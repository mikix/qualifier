{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d906eb88-7739-4eb2-8a74-ae160803712f",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Qualifier Quality Metrics\"\n",
    "jupyter: python3\n",
    "execute:\n",
    "    echo: false\n",
    "    warning: false\n",
    "    eval: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46723ebb-4c9f-4592-9e0c-20b156516dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| output: false\n",
    "%pip install sqlalchemy\n",
    "# be sure to adjust this if the duckdb version in the dbt driver is differs\n",
    "%pip install duckdb==0.5.1\n",
    "%pip install duckdb-engine\n",
    "%pip install psycopg2\n",
    "%pip install python-dotenv\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "from IPython.display import display, Markdown, HTML\n",
    "from datetime import datetime, timezone\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9df1347-aa04-435f-97d7-793ccb072570",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| tags: [parameters]\n",
    "\n",
    "load_dotenv(override=True)\n",
    "connection_string = os.environ.get('DB_CONNECTION_STRING')\n",
    "connection_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6415f048-a2b3-4d3b-ba48-73a428bdc967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.pool import NullPool\n",
    "con = create_engine(connection_string, poolclass=NullPool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4033df-e5a9-40ee-8a38-86f45c710202",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_time = con.execute(\"SELECT MAX(run_started_at) FROM quality_metrics\").fetchall()\n",
    "as_of_time = metric_time[0][0]\n",
    "\n",
    "if as_of_time:\n",
    "    utc_dt = datetime.fromisoformat(as_of_time)\n",
    "    utc_dt = utc_dt.replace(tzinfo=timezone.utc).astimezone(tz=None)\n",
    "    display(Markdown(\"## Run at \" + utc_dt.strftime(\"%b %d, %Y at %-I:%M:%S %p\")))\n",
    "else:\n",
    "    display(Markdown(\"## No quality metrics found\"))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e747a4c6-d592-4f89-9e8a-0105a933556f",
   "metadata": {},
   "source": [
    "### Error thershold settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0806b63c-98f0-4c25-b067-f1fb28907474",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_error_threshold = 0.05\n",
    "error_thresholds = {\n",
    "    \"q_obs_comp_value_range\": default_error_threshold,\n",
    "    \"q_obs_value_range\": default_error_threshold\n",
    "}\n",
    "error_threshold_sql_fragments = []\n",
    "for k,v in error_thresholds.items():\n",
    "    error_threshold_sql_fragments.append(f\"SELECT '{k}' AS metric_name, {v} AS error_threshold\")  \n",
    "error_threshold_sql = \" UNION ALL \".join(error_threshold_sql_fragments)\n",
    "thresholds = pd.read_sql(error_threshold_sql, con)\n",
    "output = thresholds.to_html(index=False)\n",
    "\n",
    "display(HTML(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68df2d6d-0bea-41ad-b68c-8946ffc0b21d",
   "metadata": {},
   "source": [
    "## Summary \n",
    "#### Error rate (by metric and critera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8a8fb1-3883-4e4a-b079-3574c2f1869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "    WITH error_thresholds AS ({error_threshold_sql}),\n",
    "    summary_metrics AS (\n",
    "        SELECT \n",
    "            metric_name,\n",
    "            criteria,\n",
    "            SUM(error_count)::numeric AS error_count,\n",
    "            SUM(denominator)::numeric AS denominator\n",
    "        FROM quality_metrics\n",
    "        WHERE run_started_at = (SELECT MAX(run_started_at) FROM quality_metrics)\n",
    "        GROUP BY 1,2\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "        summary_metrics.metric_name, \n",
    "        criteria, \n",
    "        error_count::INTEGER AS errors, \n",
    "        denominator::INTEGER AS denominator,\n",
    "        (error_count/denominator) AS \"error rate\"\n",
    "    FROM summary_metrics\n",
    "    LEFT JOIN error_thresholds ON\n",
    "        summary_metrics.metric_name = error_thresholds.metric_name\n",
    "    WHERE\n",
    "        (error_count/denominator)::DECIMAL >= COALESCE(error_threshold, {default_error_threshold})\n",
    "\"\"\"\n",
    "summary = pd.read_sql(sql, con)\n",
    "output = summary.to_html(formatters={\n",
    "    'errors': '{:,}'.format, \n",
    "    'denominator': '{:,}'.format, \n",
    "    'error rate': '{:,.0%}'.format\n",
    "}, index=False)\n",
    "\n",
    "display(HTML(output))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2db47b7-19f7-4f9e-a0e1-a03af658d3de",
   "metadata": {},
   "source": [
    "#### Error Rate over Time (by metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa085c5-33c5-42f6-bc59-d520ad931e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "sql = f\"\"\"\n",
    "SELECT \n",
    "    run_started_at::TIMESTAMP AS run_started_at,\n",
    "    metric_name,\n",
    "    ROUND((SUM(error_count)::NUMERIC/SUM(denominator)::NUMERIC)*100) AS error_rate\n",
    "FROM quality_metrics\n",
    "GROUP BY 1,2\n",
    "\"\"\"\n",
    "metric_errors = pd.read_sql(sql, con)\n",
    "pivot = metric_errors.pivot(index=\"run_started_at\", columns=\"metric_name\", values=\"error_rate\")\n",
    "chart = pivot.plot.line(marker=\"o\", xlabel=\"run started at\", ylabel=\"error count\", title=\"Errors\")\n",
    "chart.legend(title=\"metric\")\n",
    "chart.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%b-%d %H:%M'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff72bfe-d454-44de-bdc7-de9ec07f56aa",
   "metadata": {},
   "source": [
    "### Errors Above Threshold (by metric and criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac54746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_to_table(metric_name):\n",
    "    sql = f\"\"\"\n",
    "        WITH error_thresholds AS ({error_threshold_sql}),\n",
    "        summary_metrics AS (\n",
    "            SELECT \n",
    "                metric_name,\n",
    "                criteria,\n",
    "                error_examples,\n",
    "                SUM(error_count)::numeric AS error_count,\n",
    "                SUM(denominator)::numeric AS denominator\n",
    "            FROM quality_metrics\n",
    "            WHERE run_started_at = (SELECT MAX(run_started_at) FROM quality_metrics)\n",
    "                AND metric_name = '{metric_name}'\n",
    "            GROUP BY 1,2,3\n",
    "        )\n",
    "\n",
    "        SELECT\n",
    "            criteria, \n",
    "            error_count::INTEGER AS errors, \n",
    "            denominator::INTEGER AS denominator,\n",
    "            (error_count/denominator) AS \"error rate\",\n",
    "            error_examples AS examples\n",
    "        FROM summary_metrics\n",
    "        LEFT JOIN error_thresholds ON\n",
    "            summary_metrics.metric_name = error_thresholds.metric_name\n",
    "        WHERE\n",
    "            (error_count/denominator)::DECIMAL >= COALESCE(error_threshold, {default_error_threshold})\n",
    "    \"\"\"\n",
    "    detail = pd.read_sql(sql, con)\n",
    "    return detail.to_html(formatters={\n",
    "        'errors': '{:,}'.format, \n",
    "        'denominator': '{:,}'.format, \n",
    "        'error rate': '{:,.0%}'.format\n",
    "    }, index=False)\n",
    "\n",
    "\n",
    "sql = f\"\"\"\n",
    "SELECT \n",
    "    metric_name\n",
    "FROM quality_metrics\n",
    "GROUP BY 1\n",
    "\"\"\"\n",
    "metrics = pd.read_sql(sql, con)\n",
    "\n",
    "for metric_name in metrics[\"metric_name\"]:\n",
    "    display(Markdown(\"### \" + metric_name))\n",
    "    display(HTML(metric_to_table(metric_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741c6b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| output: false\n",
    "# close the database\n",
    "con.dispose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8cc24feb2bc644aa7a4bee05b98787ee7c852ae4b5e6aed3858f74310048c407"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
